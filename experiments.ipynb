{"cells":[{"cell_type":"markdown","metadata":{"id":"s0aWbCIHjOWe"},"source":["## Create and compile the model - Centralised approach - deadline 26 June\n","\n","The model will be later used for training on each client.\n","\n","Target 1: Train a centralised model and calculate performance.\n","\n","Target 2: Test various models (imagenet etc.)\n","\n","Target 3: Write documentation about centralised model."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22663,"status":"ok","timestamp":1700653812419,"user":{"displayName":"Theodoros Spyridopoulos","userId":"15874316622970173448"},"user_tz":0},"id":"zY97wJqXjOWg","outputId":"b26a17c9-1b65-4d10-dc8e-b8a8a920b471"},"outputs":[{"name":"stdout","output_type":"stream","text":["Wed Nov 22 11:49:54 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   39C    P0    26W / 300W |      0MiB / 16384MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 170498071/170498071 [00:13<00:00, 12840362.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","Number of training images:  40000\n","Number of validation images:  10000\n","Number of test images:  10000\n"]}],"source":["import torch\n","from torchvision import datasets, transforms    # import datasets and transforms from torchvision\n","import matplotlib.pyplot as plt    # import matplotlib.pyplot to plot images\n","import numpy as np  # import numpy to perform numerical operations\n","from tqdm.notebook import tqdm  # import tqdm to display progress bar\n","\n","# Set the device for google colaboratory\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)\n","\n","device = \"cuda\"\n","\n","#print(f\"Using device: {device}\")\n","\n","#device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n","#print(f\"Using device: {device}\")\n","\n","\n","#################################### Data Loading ##########################################################\n","############################################################################################################\n","\n","batch_size = 100 # Define batch size\n","\n","# Define a transform to normalize the data\n","# Use compose to chain multiple transforms together\n","transform = transforms.Compose([transforms.ToTensor(),  # Convert the image to PyTorch tensor\n","                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # Normalize the data with mean and standard deviation\n","                                ])\n","\n","# Download the training data and test data\n","trainset = datasets.CIFAR10(root='./data', download=True, transform=transform) # CIFAR10 is a dataset of natural images belonging to 10 different classes\n","testset = datasets.CIFAR10(root='./data', download=True, train=False, transform=transform) #    train=False means we want the test data and not the training data\n","\n","# Split the training data into training and validation sets\n","trainset, valset = torch.utils.data.random_split(trainset, [40000, 10000]) # 40000 is the number of images in the training set and 10000 is the number of images in the validation set\n","\n","# Print the number of images in the training and test sets\n","print(\"Number of training images: \", len(trainset))\n","print(\"Number of validation images: \", len(valset))\n","print(\"Number of test images: \", len(testset))\n","\n","# Create a dataloader to load the data in batches\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)    # shuffle=True means the data will be shuffled at every epoch\n","                                                                                    # batch_size=64 means we will load 64 images at a time\n","# Create a dataloader to load the validation data in batches\n","valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)     # We use a dataloader to load the data in batches and improve validation performance\n","                                                                                    # shuffle=False means the data will not be shuffled since we want to validate on the validation data\n","                                                                                    # batch_size=64 means we will load 64 images at a time\n","\n","# Create a dataloader to load the test data in batches\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)     # We use a dataloader to load the data in batches and improve test performance\n","                                                                                    # shuffle=False means the data will not be shuffled since we want to test on the test data\n","                                                                                    # batch_size=64 means we will load 64 images at a time"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["949a5a411897487486d7953278fa90d7"]},"id":"_tqY7RcI3TOx","outputId":"b12efd40-5c4e-4af5-ada5-84090d83b1bc"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using cache found in /Users/scmts1/.cache/torch/hub/pytorch_vision_v0.6.0\n","/Users/scmts1/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/Users/scmts1/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Using cache found in /Users/scmts1/.cache/torch/hub/pytorch_vision_v0.6.0\n","/Users/scmts1/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /Users/scmts1/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"949a5a411897487486d7953278fa90d7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0.00/83.3M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): Identity()\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=1)\n","  (fc): Linear(in_features=512, out_features=10, bias=True)\n",")\n"]}],"source":["#################################### Model Structure #######################################################\n","############################################################################################################\n","\n","# Load VGGNet16 model from torchvision and modify it to accept 32x32 images and 3 channels\n","# model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg16', pretrained=True) # pretrained=True means we want to use the pretrained weights of the model\n","# model.features[0] = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1) # 3 is the number of input channels and 64 is the number of output channels\n","# model.classifier[6] = torch.nn.Linear(4096, 10) # 4096 is the number of input features and 10 is the number of output features\n","\n","# Load the resnet18 model\n","resnet18_model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True) # pretrained=True means we want to use the pretrained weights of the model\n","\n","# Load the resnet34 model\n","resnet34_model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet34', pretrained=True) # pretrained=True means we want to use the pretrained weights of the model\n","\n","# Create a function that modifies resnet18 and resnet34 so that they can be applied to CIFAR10 dataset\n","def modify_resnet(model):\n","    model.conv1 = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False) # 3 is the number of input channels and 64 is the number of output channels\n","    model.maxpool = torch.nn.Identity() # Remove the maxpool layer\n","    model.avgpool = torch.nn.AdaptiveAvgPool2d(1) # Add an adaptive average pooling layer\n","    model.fc = torch.nn.Linear(512, 10) # 512 is the number of input features and 10 is the number of output features\n","    return model\n","\n","# Visualise the modified resnet18 model\n","resnet18_model = modify_resnet(resnet18_model)\n","print(resnet18_model)\n","\n","\n","\n","# Modify the last layer of the model to have 10 outputs instead of 1000\n","#model.fc = torch.nn.Linear(512, 10) # 512 is the number of input features and 10 is the number of output features\n","# Modify the first layer of the model to accept 32x32 images instead of 224x224 images and 3 channels instead of 1 channel\n","#model.conv1 = torch.nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False) # 3 is the number of input channels and 64 is the number of output channels\n","############################################################################################################\n","############################################################################################################import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G8yLO78n3TOy"},"outputs":[],"source":["\n","#################################### Training ##############################################################\n","############################################################################################################\n","\n","model.to(device)\n","\n","# Define the loss function and optimizer\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.7)\n","\n","# Initialize lists to store the losses and accuracies\n","train_losses = []\n","val_losses = []\n","val_accuracies = []\n","\n","# Train the model\n","for epoch in range(4):\n","    running_loss = 0.0\n","    for i, data in enumerate(tqdm(trainloader, desc=f'Epoch {epoch+1}'), 0):\n","        inputs, labels = data\n","\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","        # Calculate validation loss and accuracy every 50 batches\n","        if i % 50 == 49:\n","            val_loss = 0.0\n","            correct = 0\n","            total = 0\n","            with torch.no_grad():\n","                for data in valloader:\n","                    images, labels = data\n","\n","                    images = images.to(device)\n","                    labels = labels.to(device)\n","\n","                    outputs = model(images)\n","                    val_loss = criterion(outputs, labels)\n","                    _, predicted = torch.max(outputs.data, 1)\n","                    total += labels.size(0)\n","                    correct += (predicted == labels).sum().item()\n","                    val_losses.append(val_loss)\n","                    val_accuracies.append(100 * correct / total)\n","\n","            avg_loss = running_loss / 50\n","            train_losses.append(avg_loss)\n","            running_loss = 0.0\n","\n","    print(f'Epoch {epoch+1} training loss: {avg_loss:.3f} validation accuracy: {100 * correct / total:.2f}%')\n","\n","print('Finished Training')\n","torch.save(model.state_dict(), './VGGNet16.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BwQC30NKAhme"},"outputs":[],"source":["\n","# Create a figure for plotting the training loss\n","plt.figure()\n","plt.title('Training Loss')\n","plt.xlabel('100Minibatches')\n","plt.ylabel('Loss')\n","plt.plot(np.arange(len(losses)), losses) # Plot the losses vs the number of mini-batches\n","\n","val_losses = [val.cpu() for val in val_losses]\n","# Create a figure for plotting the validation loss\n","plt.figure()\n","plt.title('Validation Loss')\n","plt.xlabel('100Minibatches')\n","plt.ylabel('Loss')\n","plt.plot(np.arange(len(val_losses)), val_losses) # Plot the validation losses vs the number of mini-batches\n","\n","# Create a figure for plotting the validation accuracy\n","plt.figure()\n","plt.title('Validation Accuracy')\n","plt.xlabel('100Minibatches')\n","plt.ylabel('Accuracy')\n","plt.plot(np.arange(len(val_accuracies)), val_accuracies) # Plot the validation accuracies vs the number of mini-batches\n","\n","# test the model on the test data\n","correct = 0\n","total = 0\n","with torch.no_grad():   # We don't need to calculate gradients since we are not training the model\n","    for data in testloader: # Iterate over the test data\n","        images, labels = data[0].to(device), data[1].to(device) # Get the inputs and labels from the data and move them to the device\n","        outputs = model(images) # Get the outputs from the model\n","        _, predicted = torch.max(outputs.data, 1) # Get the predicted labels\n","        total += labels.size(0) # Increment the total number of images\n","        correct += (predicted == labels).sum().item() # Increment the number of correctly predicted images\n","\n","print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total)) # Print the accuracy of the model on the test data"]},{"cell_type":"markdown","metadata":{"id":"lfwbgEvNt0I2"},"source":["To use k-fold cross validation in your code, you can modify the training loop to use k-fold cross validation instead of training on the entire training set. Here's an example of how you can modify the code to use k-fold cross validation:\n","\n","```python\n","import torch\n","from torchvision import datasets, transforms\n","from sklearn.model_selection import KFold\n","import numpy as np\n","\n","# Define a transform to normalize the data\n","transform = transforms.Compose([transforms.ToTensor(),\n","                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","                                ])\n","\n","# Download the data\n","trainset = datasets.CIFAR10(root='./data', download=True, transform=transform)\n","testset = datasets.CIFAR10(root='./data', download=True, train=False, transform=transform)\n","\n","# Define the number of folds\n","k = 5\n","\n","# Create a KFold object to split the data into k folds\n","kf = KFold(n_splits=k, shuffle=True)\n","\n","# Define the model\n","model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg16', pretrained=True)\n","model.features[0] = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n","model.classifier[6] = torch.nn.Linear(4096, 10)\n","model.to(device)\n","\n","# Define the loss function and optimizer\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","# Define a list to store the accuracies for each fold\n","accuracies = []\n","\n","# Loop over the folds\n","for fold, (train_indices, val_indices) in enumerate(kf.split(trainset)):\n","    print(f\"Fold {fold+1}/{k}\")\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I7fuJycdt0I2"},"outputs":[],"source":["# Create a dataloader for the training data for this fold\n","trainloader = torch.utils.data.DataLoader(torch.utils.data.Subset(trainset, train_indices), batch_size=64, shuffle=True)\n","\n","# Create a dataloader for the validation data for this fold\n","valloader = torch.utils.data.DataLoader(torch.utils.data.Subset(trainset, val_indices), batch_size=64, shuffle=False)\n","\n","# Train the model for this fold\n","for epoch in range(10):\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","    print(f\"Epoch {epoch+1}: Training loss = {running_loss/len(trainloader)}\")\n","\n","    # Evaluate the model on the validation data for this fold\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data in valloader:\n","            images, labels = data[0].to(device), data[1].to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    accuracy = 100 * correct / total\n","    print(f\"Epoch {epoch+1}: Validation accuracy = {accuracy}%\")\n","\n","#"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tX_mvLZMWyby"},"outputs":[],"source":["#print F1 score and confusion matrix\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import confusion_matrix\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","import numpy as np\n","import seaborn as sn\n","from torchvision import datasets, transforms    # import datasets and transforms from torchvision\n","\n","\n","device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")\n","\n","# set model from saved model\n","model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg16', pretrained=True)\n","model.features[0] = torch.nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n","model.classifier[6] = torch.nn.Linear(4096, 10)\n","model.load_state_dict(torch.load('./VGGNet16.pth'))\n","model.to(device) # Move the model to the device\n","\n","# Define a transform to normalize the data\n","# Use compose to chain multiple transforms together\n","transform = transforms.Compose([transforms.ToTensor(),  # Convert the image to PyTorch tensor\n","                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # Normalize the data with mean and standard deviation\n","                                ])\n","\n","# Download the training data and test data\n","testset = datasets.CIFAR10(root='./data', download=True, train=False, transform=transform) #    train=False means we want the test data and not the training data\n","\n","\n","print(\"Number of test images: \", len(testset))\n","                                                                                   # batch_size=64 means we will load 64 images at a time\n","# Create a dataloader to load the test data in batches\n","testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)     # We use a dataloader to load the data in batches and improve test performance\n","\n","\n","\n","\n","# test the model on the test data\n","correct = 0\n","total = 0\n","y_true = []\n","y_pred = []\n","\n","with torch.no_grad():   # We don't need to calculate gradients since we are not training the model\n","    for data in testloader: # Iterate over the test data\n","        images, labels = data[0].to(device), data[1].to(device) # Get the inputs and labels from the data and move them to the device\n","        outputs = model(images) # Get the outputs from the model\n","        _, predicted = torch.max(outputs.data, 1) # Get the predicted labels\n","        total += labels.size(0) # Increment the total number of images\n","        correct += (predicted == labels).sum().item() # Increment the number of correctly predicted images\n","        y_true += labels.tolist()\n","        y_pred += predicted.tolist()\n","\n","print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total)) # Print the accuracy of the model on the test data\n","\n","# Print the F1 score\n","print('F1 score: ', f1_score(y_true, y_pred, average='macro'))\n","\n","# visualise the confusion matrix as a heatmap\n","cm = confusion_matrix(y_true, y_pred)\n","df_cm = pd.DataFrame(cm, range(10), range(10))\n","plt.figure(figsize=(10,7))\n","sn.set(font_scale=1.4) # for label size\n","sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 12}) # font size\n","plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"iMWyFehSjOWi"},"source":["## Implement model attacks\n","\n","Implement a function to simulate a model attack and measure the effect of the attack against existing methods\n","\n","Target 1: Perform literature review on simulating model attacks (ART seems to be the best option, but this is to simulate the actual attack; perhaps most papers use other simpler ways to implement an attack)\n","\n","Target 2: Implement model attack simulation based on literature review performed in target 1\n","\n","Target 3: Measure model performance (using the model that was trained in the previous stage) before and after the attack"]},{"cell_type":"markdown","metadata":{"id":"jbmjUI5ZjOWi"},"source":["## Implement Federated Learning architecture\n","\n","Implement FL relying on the existing models from stage 1\n","\n","Target 1: Perform literature review on existing FL frameworks with a focus on those used for cybersecurity experimentation on model attacks\n","\n","Target 2: Implement FL relying on the model from stage 1\n","\n","Target 3: Train and evaluate the results on each client"]},{"cell_type":"markdown","metadata":{"id":"_WyLWcE-jOWi"},"source":["## Implement Federated Learning attack\n","\n","Target 1: Perform literature review on model attacks in Federated Learning (you already have the model attack form earlier stage, but there might be better ways to attack in order to make the attack persistent for example...)\n","\n","Target 2: Implement the attack(s) and measure the performance of the model (on each client and the Global model) in time"]},{"cell_type":"markdown","metadata":{"id":"zfQUUVBajOWj"},"source":["## Implement Defence and measure effectiveness\n","\n","Target 1: Implement the proposed defence\n","\n","Target 2: Measure performance on each client and centrally for each type of attack in the previous step after applying the proposed security mechanism."]},{"cell_type":"markdown","metadata":{"id":"HCLZ2YjrjOWj"},"source":["## Compare against other defence methods"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3.9 (tensorflow)","language":"python","name":"tensorflow"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4,"widgets":{"application/vnd.jupyter.widget-state+json":{}}},"nbformat":4,"nbformat_minor":0}
